import torch


def search_similar(query_vec, db_embeddings, top_k=5):
    """
    cosine similarity ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
    """
    if not db_embeddings:
        print("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []

    if len(db_embeddings) < top_k:
        top_k = len(db_embeddings)
        print(f"âš ï¸ DBì— {len(db_embeddings)}ê°œ ì´ë¯¸ì§€ë§Œ ìˆì–´ì„œ top_kë¥¼ {top_k}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")

    try:
        # ëª¨ë“  ì„ë² ë”©ì˜ ì°¨ì›ì„ ì¼ê´€ë˜ê²Œ ë§Œë“¤ê¸°
        db_vecs = []
        for key, vec in db_embeddings.items():
            # ì°¨ì› ì •ë¦¬: 1D ë²¡í„°ë¡œ í†µì¼
            if vec.dim() > 1:
                vec = vec.squeeze()
            vec = vec.flatten()
            db_vecs.append(vec)

        # ëª¨ë“  ì„ë² ë”©ì„ ìŠ¤íƒìœ¼ë¡œ ê²°í•©
        db_tensor = torch.stack(db_vecs)
        file_list = list(db_embeddings.keys())

        # query_vec ì°¨ì› ì •ë¦¬
        if query_vec.dim() > 1:
            query_vec = query_vec.squeeze()
        query_vec = query_vec.flatten()

        print(f"ğŸ” ê²€ìƒ‰ ì •ë³´:")
        print(f"   Query shape: {query_vec.shape}")
        print(f"   DB shape: {db_tensor.shape}")

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        sims = torch.nn.functional.cosine_similarity(query_vec.unsqueeze(0), db_tensor, dim=1)

        # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
        if top_k > 0:
            top_scores, top_indices = torch.topk(sims, k=top_k, largest=True)
            results = [(file_list[idx.item()], score.item()) for idx, score in zip(top_indices, top_scores)]
        else:
            results = []

        return results

    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if 'query_vec' in locals():
            print(f"Query vector shape: {query_vec.shape}")
        if 'db_tensor' in locals():
            print(f"DB tensor shape: {db_tensor.shape}")
        return []